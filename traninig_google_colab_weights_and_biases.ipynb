{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "traninig_google_colab_weights_and_biases.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXkSsfYHEA0F"
      },
      "outputs": [],
      "source": [
        "!apt install python3.8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install virtualenv"
      ],
      "metadata": {
        "id": "W89cDMARGTF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!virtualenv -p python3.8 new_p3_env"
      ],
      "metadata": {
        "id": "CP0tAhOWGF95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source new_p3_env/bin/activate; pip3 install -r requirements.txt"
      ],
      "metadata": {
        "id": "9QbTxgLEGJHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHWE0lspHyg9",
        "outputId": "7d15807b-20d0-47bc-8682-8fc1396cc6b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Log in to your W&B account\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "ZXhR_MRrF7Wy",
        "outputId": "957f2ed0-1b9d-4dc9-c6f7-55d144e55a45"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source new_p3_env/bin/activate; python3 train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmcOiZlDHPtA",
        "outputId": "589de698-fe2e-484d-e534-56ff9559d9a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "/content/new_p3_env/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Parameter 'function'=<bound method DataModule.tokenize_data of <data.DataModule object at 0x7f8e06975d90>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "100% 9/9 [00:00<00:00, 20.19ba/s]\n",
            "100% 2/2 [00:00<00:00, 32.54ba/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmremreozan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220527_153612-209tlkgg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdandy-puddle-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mremreozan/mlops_graviraja\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mremreozan/mlops_graviraja/runs/209tlkgg\u001b[0m\n",
            "\n",
            "  | Name                   | Type                          | Params\n",
            "-------------------------------------------------------------------------\n",
            "0 | bert                   | BertForSequenceClassification | 4.4 M \n",
            "1 | train_accuracy_metric  | Accuracy                      | 0     \n",
            "2 | val_accuracy_metric    | Accuracy                      | 0     \n",
            "3 | f1_metric              | F1                            | 0     \n",
            "4 | precision_macro_metric | Precision                     | 0     \n",
            "5 | recall_macro_metric    | Recall                        | 0     \n",
            "6 | precision_micro_metric | Precision                     | 0     \n",
            "7 | recall_micro_metric    | Recall                        | 0     \n",
            "-------------------------------------------------------------------------\n",
            "4.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.4 M     Total params\n",
            "17.545    Total estimated model params size (MB)\n",
            "Epoch 0:  93% 140/151 [01:31<00:07,  1.53it/s, loss=0.617, v_num=lkgg, valid/loss_epoch=0.650, valid/acc=0.648, valid/precision_macro=0.324, valid/recall_macro=0.500, valid/precision_micro=0.648, valid/recall_micro=0.648, valid/f1=0.648, train/loss_step=0.523, train/acc_step=0.812]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: 100% 151/151 [01:36<00:00,  1.56it/s, loss=0.605, v_num=lkgg, valid/loss_epoch=0.618, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.609, train/acc_step=0.718, train/loss_epoch=0.612, train/acc_epoch=0.704, valid/loss_step=0.491]\n",
            "Epoch 1:  93% 140/151 [01:31<00:07,  1.53it/s, loss=0.596, v_num=lkgg, valid/loss_epoch=0.618, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.570, train/acc_step=0.750, train/loss_epoch=0.612, train/acc_epoch=0.704, valid/loss_step=0.491]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: 100% 151/151 [01:36<00:00,  1.56it/s, loss=0.606, v_num=lkgg, valid/loss_epoch=0.620, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.669, train/acc_step=0.641, train/loss_epoch=0.608, train/acc_epoch=0.704, valid/loss_step=0.480]\n",
            "Epoch 2:  93% 140/151 [01:30<00:07,  1.55it/s, loss=0.608, v_num=lkgg, valid/loss_epoch=0.620, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.597, train/acc_step=0.719, train/loss_epoch=0.608, train/acc_epoch=0.704, valid/loss_step=0.480]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 151/151 [01:35<00:00,  1.58it/s, loss=0.6, v_num=lkgg, valid/loss_epoch=0.617, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.568, train/acc_step=0.744, train/loss_epoch=0.607, train/acc_epoch=0.704, valid/loss_step=0.494]  \n",
            "Epoch 3:  93% 140/151 [01:30<00:07,  1.55it/s, loss=0.604, v_num=lkgg, valid/loss_epoch=0.617, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.669, train/acc_step=0.641, train/loss_epoch=0.607, train/acc_epoch=0.704, valid/loss_step=0.494]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: 100% 151/151 [01:35<00:00,  1.59it/s, loss=0.609, v_num=lkgg, valid/loss_epoch=0.616, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.606, train/acc_step=0.692, train/loss_epoch=0.606, train/acc_epoch=0.704, valid/loss_step=0.495]\n",
            "Epoch 4:  93% 140/151 [01:30<00:07,  1.54it/s, loss=0.621, v_num=lkgg, valid/loss_epoch=0.616, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.680, train/acc_step=0.609, train/loss_epoch=0.606, train/acc_epoch=0.704, valid/loss_step=0.495]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: 100% 151/151 [01:35<00:00,  1.58it/s, loss=0.601, v_num=lkgg, valid/loss_epoch=0.613, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.559, train/acc_step=0.769, train/loss_epoch=0.604, train/acc_epoch=0.704, valid/loss_step=0.513]\n",
            "Epoch 4: 100% 151/151 [01:35<00:00,  1.58it/s, loss=0.601, v_num=lkgg, valid/loss_epoch=0.613, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.559, train/acc_step=0.769, train/loss_epoch=0.604, train/acc_epoch=0.704, valid/loss_step=0.513]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch ▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             global_step ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/acc_epoch ▁████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/acc_step ▄▅▂▅▇▆▃▅▃▃▅▅▁▄█▅▅▃▃▄▅▃▃▂▃▅▁▅▃▅▄▅▆▅▁▃▃▄▇▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/loss_epoch █▄▄▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/loss_step ▆▄▇▃▂▃▇▄▅▆▅▄█▅▁▄▃▆▅▅▄▆▆▇▆▄█▄▆▄▅▄▃▃█▅▅▅▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/global_step ▁▂▂▂▁▁▁▁▃▃▃▄▁▁▁▁▄▅▅▁▁▁▁▁▆▆▆▁▁▁▁▁▇▇█▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               valid/acc ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                valid/f1 ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/loss_epoch ▇█▅▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid/loss_step/epoch_0 ▆█▇▆▆▃▁█▅█▇▆▄▆▄▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid/loss_step/epoch_1 ▆█▇▆▆▃▁█▅█▇▆▄▆▄▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid/loss_step/epoch_2 ▆█▇▆▆▃▁█▅█▇▆▄▆▄▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid/loss_step/epoch_3 ▆█▇▆▆▃▁█▅█▇▆▄▆▄▆▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid/loss_step/epoch_4 ▆█▇▆▆▃▁█▆█▇▆▅▆▄▆▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/precision_macro ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/precision_micro ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      valid/recall_macro ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      valid/recall_micro ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             global_step 669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/acc_epoch 0.70436\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/acc_step 0.76923\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/loss_epoch 0.60424\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/loss_step 0.55931\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/global_step 669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               valid/acc 0.69128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                valid/f1 0.69128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        valid/loss_epoch 0.61301\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid/loss_step/epoch_0 0.49062\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid/loss_step/epoch_1 0.47982\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid/loss_step/epoch_2 0.49384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid/loss_step/epoch_3 0.49534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: valid/loss_step/epoch_4 0.51269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/precision_macro 0.34564\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   valid/precision_micro 0.69128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      valid/recall_macro 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      valid/recall_micro 0.69128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mdandy-puddle-3\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/mremreozan/mlops_graviraja/runs/209tlkgg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 12 media file(s), 3 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220527_153612-209tlkgg/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source new_p3_env/bin/activate; python3 inference.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nhU7cGGIEBS",
        "outputId": "7740c3e2-1c5c-4d13-bda8-7b81680105f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[{'label': 'unacceptable', 'score': [1.0, 1.0]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H-fRVxv6OHls"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}